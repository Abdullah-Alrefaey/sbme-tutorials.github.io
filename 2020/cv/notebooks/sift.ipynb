{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale invariant feature transform (SIFT) \n",
    "\n",
    "\n",
    "This tutorial mainly follows:\n",
    "\n",
    "\n",
    "1. The original paper (a recommended read): [{Distinctive Image Featuresfrom Scale-Invariant Keypoints}](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)\n",
    "2. Another tutorial at [{AI Shack | SIFT: Theory and Practice}](https://aishack.in/tutorials/sift-scale-invariant-feature-transform-introduction/) with adapting to new data and other considrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem definition\n",
    "\n",
    "* Features or **key-points** of an image are corners which are unique in the image.\n",
    "* Harris is a corner detector, we have discussed Previously. \n",
    "* Corner detectors are invariant for translation, illumination and rotation. **But it is variant for scaling**. \n",
    "\n",
    "### Example\n",
    "\n",
    "Next figure shows two different scales of same image. \n",
    "\n",
    "<span style=\"display:block;text-align:center\"><img style=\"width:75%\"  src=\"media/sift_scale_invariant.jpg\"></span>\n",
    "\n",
    "* In smaller image,  it's easy to detect that there is a corner, but what about same image in the large scale. It will be difficult to detect that corner so this feature point will not be recognized for all scales. \n",
    "* So size of the window will effect the detection of corners. Large corners needs large windows and smaller corners needs smaller windows. \n",
    "\n",
    "Scale invariant feature descriptor (SIFT) is not a new way to find key-points or corners that is invariant to scale. But it is a descriptor of detected corners of different image scales or image pyramids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Image pyramids and scale-spaces\n",
    "\n",
    "Image pyramids or image scale space is the proposed method to handle images in different scales. We have different scale-spaces \n",
    "\n",
    "* Gaussian scale space (Gaussian pyramid)\n",
    "* Laplacian of gaussian (LOG) scale space \n",
    "* Difference of gaussian (DOG) scale space \n",
    "\n",
    "The basic idea to build scale space is shown in the following figure \n",
    "\n",
    "**Gaussian Pyramid**\n",
    "\n",
    "<span style=\"display:block;text-align:center\"><img style=\"width:55%\"  src=\"media/Image_pyramid.png\"></span>\n",
    "\n",
    "\n",
    "[source](https://en.wikipedia.org/wiki/Pyramid_(image_processing))\n",
    "\n",
    "**LOG Pyramid**\n",
    "\n",
    "<span style=\"display:block;text-align:center\"><img style=\"width:85%\"  src=\"media/LOG.png\"></span>\n",
    "\n",
    "\n",
    "In SIFT we usually prefer DOG scale space which is an approximate of LOG and simpler in calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT scale space (DOG)\n",
    "\n",
    "In SIFT Pyramid we have \n",
    "\n",
    "* Octaves: different levels of image resolutions (pyramids levels)\n",
    "* Scales: different scales of window in each octave level (different $\\sigma$ of gaussian window)\n",
    "\n",
    "![](media/sift_dog.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical details of constructing the scale space in SIFT\n",
    "\n",
    "The following practices suggested by the SIFT author ([{Distinctive Image Featuresfrom Scale-Invariant Keypoints}](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf) p. 7):\n",
    "\n",
    "\n",
    "1. Overall, we consider four octaves (resolution levels, or image shapes). Each octave shape is $\\times 2$ subsampling of its previous octave. So if first octave resolution is (64,64), the next resolutions are (32,32), (16,16), and (8,8).\n",
    "1. For each octave, we consider five different blurred images ($\\sigma, k \\sigma, k^2 \\sigma, k^3 \\sigma, k^4 \\sigma$); and $k=\\sqrt{2}$. Hence, the squence of Gaussian blurring becomes ($\\sigma,\\sqrt{2} \\sigma, 2 \\sigma, 2\\sqrt{2} \\sigma, 4 \\sigma$).\n",
    "1. We start the first octave on a $\\times 2$ upsampled from the input image. This allows extracting features on a ~~larger~~ smaller scale (because the relative size of Harris operator to the image decreases). So if the input image is (128,128), the four octaves will have the following resolutions: (256,256), (128,128), (64,64), and (32,32).\n",
    "1. For the $2^{nd}-$, $3^{rd}-$, and $4^{th}-$octaves, we $\\times 2$ subsample the image from the previous octave that has twice $\\sigma$ of the initial image (which mean the third image). \n",
    "1. Finally, from each octave, we obtain 4 DoG images.\n",
    "1. $\\sigma = 1.6$ is chosen by the author for stable results (repeatability).\n",
    "\n",
    "##### Let's declare the previous practical considerations before starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "In  \u001b[0;34m[1]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mscipy\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msignal\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m convolve2d\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from scipy.signal import convolve2d\n",
    "from cvutils import gaussian_kernel2d\n",
    "from skimage.transform import rescale\n",
    "\n",
    "# The following are suggested by SIFT author\n",
    "N_OCTAVES = 4 \n",
    "N_IMAGES_PER_OCTAVE = 5 \n",
    "SIGMA = lambda s: [ (2.0**i)*s for i in range(5) ] # (s, √2s , 2s, 2√2 s , 4s )\n",
    "SIGMA_SIFT = SIGMA(1.6) #\n",
    "KERNELS_SIFT = [ gaussian_kernel2d(std = s, kernlen = 4 * int(s+0.5) + 1) for s in SIGMA_SIFT ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_dog( img ):\n",
    "    octaves = []\n",
    "    dog = []\n",
    "    for i in range(4):\n",
    "        base = rescale( img, 2, anti_aliasing=False) if i == 0 else rescale( octaves[i-1][2], 0.5 , anti_aliasing = False ) \n",
    "        octaves.append([ convolve2d( base , kernel , 'same') for kernel in KERNELS_SIFT ])\n",
    "        dog.append([ s2 - s1 for (s1,s2) in zip( octaves[i][:-1], octaves[i][1:])]) # ;)\n",
    "    return dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-point (corner) scale localization\n",
    "\n",
    "For each key-point (corner) we need to find its best scale which have maximum value (cornerness measure). It is achieved by comparing same corner with its neighbors of above and lower scales and select scale with maximum value. For same iamge, it is not necessary for its corners to be localized at same scale.\n",
    "\n",
    "![](media/sift_local_extrema.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SIFT feature descriptor\n",
    "\n",
    "![](media/sift-fingerprint.jpg)\n",
    "\n",
    "[source](http://aishack.in/tutorials/sift-scale-invariant-feature-transform-features/)\n",
    "\n",
    "After localization of a key-point in our scale space. We can get its SIFT descriptor as follow\n",
    "\n",
    "* Extract a $$16 \\times 16$$ window centered by this point.\n",
    "* Get gradient magnitude and multiply it by a $$16 \\times 16$$ gaussian window of $$\\sigma =1.5$$\n",
    "* Get gradient angle direction. \n",
    "* Adjusting orientation (To be rotation invariant):\n",
    "    * Get the gradient angle of the window and Quantize them to 36 values (0, 10, 20, ..., 360)\n",
    "    * Locate dominant corner direction which is most probable angle (angle with max value in 36 bit angle histogram)\n",
    "    * subtract dominant direction from gradient angle.\n",
    "\n",
    "![](media/siftOriented.png)\n",
    "\n",
    "* Divide this $$16 \\times 16$$ patch to sixteen $$4 \\times 4$$ blocks\n",
    "* For each block get magnitude weighted angle histogram and normalize it (divide by total gradient magnitudes). \n",
    " \n",
    " angles (quantized to 8 angles [0, 45, 90, ... , 360]) based on its relevant gradient magnitude i.e (histogram of angle 0 = sum(all magnitudes with angle 0))\n",
    "\n",
    " ![](media/histangles.png)\n",
    "\n",
    "\n",
    "\n",
    "* SIFT feature descriptor will be a vector of 128 element (16 blocks $$\\times$$ 8 values from each block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature matching\n",
    "\n",
    "The basic idea of feature matching is to calculate the sum square difference between two different feature descriptors (SSD). So feature will be matched with another with minimum SSD value. \n",
    "\n",
    "$$\n",
    "SSD = \\sum (v_1 - v_2)^2\n",
    "$$\n",
    "\n",
    "where $$v_1$$ and $$v_2$$ are two feature descriptors.\n",
    "\n",
    "![](media/matcher_result1.jpg)\n",
    "\n",
    "\n",
    "### Brute-Force matcher\n",
    "\n",
    "In brute-force matcher we have to match descriptor of all features in an image to descriptors of all features in another image. It is extremely expensive as we know any brute-force algorithm will guarantee getting a solution, but doesn't guarantee getting optimal solution.\n",
    "\n",
    "### RANSAC \n",
    "\n",
    "Random sample consensus is an iterative method for estimation of parameters of a mathematical model. We will model the transformation of points in source image to destination one, and try to find an estimate of model parameters. The basic idea of RANSAC algorithm is shown in the following flow chart. \n",
    "\n",
    "<!-- ![](media/ransac.png) -->\n",
    "\n",
    "RANSAC is a robust feature matcher. For example we can model the difference between two images to a set of transformations and run RANSAC to find best model that maximize correct matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpython",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
